{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LMAES_layerwise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alsr96/LMMAES/blob/master/LMAES_layerwise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh-9_uwa64qO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class structtype():\n",
        "    pass\n",
        "\n",
        "def nw_to_vec(model,layer_idx=None):\n",
        "    n_layers = len(model.layers)\n",
        "    vector=np.empty((0,))\n",
        "    ind=np.zeros((1,))\n",
        "    sum_i=0\n",
        "    if layer_idx==None:\n",
        "        idx=range(n_layers)\n",
        "    else:\n",
        "        idx=layer_idx\n",
        "    for i in idx:\n",
        "        if len(model.layers[i].get_weights())==2:\n",
        "            weights, biases = model.layers[i].get_weights()\n",
        "            s_w=np.size(weights)\n",
        "            sum_i=sum_i+s_w\n",
        "            ind=np.append(ind,sum_i)\n",
        "            w_v=np.reshape(weights,(s_w,))\n",
        "            s_b=np.size(biases)\n",
        "            sum_i=sum_i+s_b\n",
        "            ind=np.append(ind,sum_i)\n",
        "            b_v=np.reshape(biases,(s_b,))\n",
        "            wb=np.append(w_v,b_v)\n",
        "            vector=np.append(vector,wb)\n",
        "    return vector, ind\n",
        "\n",
        "def vec_to_nw(vector,ind,model,layer_idx=None):\n",
        "    n_layers = len(model.layers)\n",
        "    if layer_idx==None:\n",
        "        idx=range(n_layers)\n",
        "    else:\n",
        "        idx=layer_idx\n",
        "    k=0\n",
        "    for i in idx:\n",
        "        if len(model.layers[i].get_weights())==2:\n",
        "            weights,biases=model.layers[i].get_weights()\n",
        "            j1=k\n",
        "            j2=k+1\n",
        "            j3=k+2\n",
        "            weights=np.reshape(vector[int(ind[j1]):int(ind[j2])],np.shape(weights))\n",
        "            biases=np.reshape(vector[int(ind[j2]):int(ind[j3])],np.shape(biases))\n",
        "            model.layers[i].set_weights((weights,biases))\n",
        "            k=k+2\n",
        "    return model\n",
        "\n",
        "#split=[15,15]\n",
        "def split_data(input_data,target2,split):\n",
        "    n_samples=len(target2)\n",
        "    s1_input=input_data[0:int(split[0]*n_samples/(split[0]+split[1]))]\n",
        "    s1_target=target2[0:int(split[0]*n_samples/(split[0]+split[1]))]\n",
        "    s2_input=input_data[int(split[0]*n_samples/(split[0]+split[1])):n_samples]\n",
        "    s2_target=target2[int(split[0]*n_samples/(split[0]+split[1])):n_samples]\n",
        "    return s1_input, s1_target, s2_input, s2_target\n",
        "\n",
        "def lmmaes(model,total_input,total_target,split=None,batch=None,n_candidates = None,sigma=1/10,function_budget=10000,function_target=None,itr_limit=20,validation_fail_limit=6,layer_idx=None):\n",
        "    if split!=None:\n",
        "        n_validation=int(split[0]*np.shape(total_input)[0]/100)\n",
        "        n_test=int(split[1]*np.shape(total_input)[0]/100)\n",
        "        validation_input=total_input[0:n_validation]\n",
        "        validation_target=total_target[0:n_validation]\n",
        "        test_input=total_input[n_validation:n_validation+n_test]\n",
        "        test_target=total_target[n_validation:n_validation+n_test]\n",
        "        train_input=total_input[n_validation+n_test:np.shape(total_input)[0]]\n",
        "        train_target=total_target[n_validation+n_test:np.shape(total_target)[0]]\n",
        "    else:\n",
        "        train_input=total_input[0]\n",
        "        train_target=total_target[0]\n",
        "        validation_input=total_input[1]\n",
        "        validation_target=total_target[1]\n",
        "        test_input=total_input[2]\n",
        "        test_target=total_target[2]\n",
        "    # converting the weights and biases to a row vector\n",
        "    y, ind = nw_to_vec(model,layer_idx=layer_idx)\n",
        "    n_dimensions=0\n",
        "    validation_fail=0\n",
        "    if layer_idx==None:\n",
        "        n_layers = len(model.layers)\n",
        "        itr = range(n_layers)\n",
        "    else:\n",
        "        n_layers = len(layer_idx)\n",
        "        itr = layer_idx\n",
        "    for i in itr:\n",
        "        if len(model.layers[i].get_weights())==2:\n",
        "            weights, biases = model.layers[i].get_weights()\n",
        "            n_dimensions=np.size(weights)+np.size(biases)+n_dimensions\n",
        "    if n_candidates==None:\n",
        "        n_candidates = 4 + np.floor(3*np.log(n_dimensions))\n",
        "    mu = np.floor(n_candidates/2)\n",
        "    w = np.empty([0,0])\n",
        "    for i in range(int(mu)):\n",
        "        w = np.append(w,np.log(mu+0.5)-np.log(i+1))\n",
        "    sum_w = np.sum(w)\n",
        "    w = w/sum_w\n",
        "    mu_w = 1/(np.sum(np.square(w)))\n",
        "    #\n",
        "    m = 4 + np.floor(3*np.log(n_dimensions))\n",
        "    c_sigma = 2*n_candidates/n_dimensions\n",
        "    const1=np.sqrt(mu_w*c_sigma*(2-c_sigma))\n",
        "    \n",
        "    c_d = np.empty([0,0])\n",
        "    c_c = np.empty([0,0])\n",
        "    const2=np.empty([0,0])\n",
        "    for i in range(int(m)):\n",
        "        c_d = np.append(c_d,1/(n_dimensions*(1.5**i)))\n",
        "        c_c = np.append(c_c,n_candidates/(n_dimensions*(4**i)))\n",
        "        const2=np.append(const2,np.sqrt(mu_w*c_c[i]*(2-c_c[i])))\n",
        "    t=0\n",
        "    func_calls=0\n",
        "    p_sigma = np.zeros((n_dimensions,))\n",
        "    m_i = np.zeros((int(m), n_dimensions))\n",
        "    if batch==None:\n",
        "        batch=len(train_target)\n",
        "    \n",
        "    itr=0\n",
        "    res=model.evaluate(x=train_input,y=train_target)\n",
        "    func_calls=func_calls+1\n",
        "    log_train=np.array([[itr,res[0],res[1]]])\n",
        "    best_model=model\n",
        "    best_res=model.evaluate(x=validation_input,y=validation_target)\n",
        "    func_calls=func_calls+1\n",
        "    log_validation=np.array([[itr,res[0],res[1],validation_fail]])\n",
        "    # for testing print(model.layers[0].get_weights(),model.layers[1].get_weights())\n",
        "    # for testing print(n_dimensions)\n",
        "    while itr<=itr_limit:\n",
        "        for k in range(int(len(train_target)/batch)):\n",
        "            #t, y, p_sigma, sigma = lm_func.step_lmmaes(t,n_dimensions,m,n_candidates,c_c,c_d,loss_func,y,sigma,mu,w,c_sigma,const1,const2,data)\n",
        "            z=np.random.randn(int(n_candidates),n_dimensions)\n",
        "            d = np.copy(z)\n",
        "            f_list=np.empty((int(m),1))\n",
        "            for i in range(int(m)):\n",
        "                for j in range(np.minimum(t,int(m))):\n",
        "                    d[i]=(1-c_d[j])*d[i] + c_d[j]*np.sum(m_i[j]**2)*d[i]#(np.matmul(m_i[j][np.newaxis],np.matmul(np.transpose(m_i[j][np.newaxis]),d[i][np.newaxis])))\n",
        "                #res = list(map(benchmark_functions.rosenbrock,[y+sigma*d[i]])) ###############\n",
        "                modeli=vec_to_nw(y+sigma*d[i],ind,model,layer_idx=layer_idx)\n",
        "                res=modeli.evaluate(x=train_input[batch*k:batch*(k+1)],y=train_target[batch*k:batch*(k+1)],verbose=0)\n",
        "                func_calls=func_calls+1\n",
        "                f_list[i][0] = res[0]\n",
        "            \n",
        "            #f_list = np.append(f_list, (np.linalg.norm(y))**2)##\n",
        "            #d = np.append(d, np.zeros((1,n_dimensions)), axis=0)##\n",
        "            #z = np.append(z, np.zeros((1,n_dimensions)), axis=0)##\n",
        "            \n",
        "            sortidx_f = f_list.argsort(axis=0)\n",
        "            #fjhyt\n",
        "            sortidx_f = sortidx_f[0:int(mu)]\n",
        "            best_list = np.empty([int(mu),n_dimensions])\n",
        "            j = 0\n",
        "            for i in sortidx_f:\n",
        "                best_list[j] = w[j]*d[i]\n",
        "                j = j+1\n",
        "            y_next = y + sigma*np.sum(best_list,0)\n",
        "            \n",
        "            best_list2 = np.empty([int(mu),n_dimensions])\n",
        "            j = 0\n",
        "            for i in sortidx_f:\n",
        "                best_list2[j] = w[j]*z[i]\n",
        "                j = j+1\n",
        "            p_sigma_next = (1-c_sigma)*p_sigma + const1*np.sum(best_list2,0)\n",
        "            mag_p_sigma_next = np.linalg.norm(p_sigma_next)\n",
        "            for i in range(int(m)):\n",
        "                m_i[i] = (1-c_c[i])*m_i[i] + const2[i]*np.sum(best_list2,0)\n",
        "            sigma_next = sigma*np.exp(c_sigma*(((mag_p_sigma_next**2)/n_dimensions)-1)/2)\n",
        "            t=t+1\n",
        "            sigma = sigma_next\n",
        "            p_sigma = p_sigma_next\n",
        "            y = y_next\n",
        "            modelt=vec_to_nw(y,ind,model,layer_idx=layer_idx)\n",
        "            jhlk=sortidx_f[0][0]\n",
        "            print(f_list[int(jhlk)],t,sigma)\n",
        "        # for testing print(model.layers[0].get_weights(),model.layers[1].get_weights())\n",
        "        # for testing dhytd\n",
        "        #trained_model=model\n",
        "        itr=itr+1\n",
        "        res=modelt.evaluate(x=train_input,y=train_target)\n",
        "        func_calls=func_calls+1\n",
        "        log_train=np.append(log_train,np.array([[itr,res[0],res[1]]]),axis=0)\n",
        "        print('train:',[itr,res[0],res[1]*100])\n",
        "        res=modelt.evaluate(x=validation_input,y=validation_target)\n",
        "        func_calls=func_calls+1\n",
        "        if res[1]>best_res[1]:\n",
        "            best_model=modelt\n",
        "            best_res=res\n",
        "            validation_fail=0\n",
        "        else:\n",
        "            validation_fail=validation_fail+1\n",
        "        log_validation=np.append(log_validation,np.array([[itr,res[0],res[1],validation_fail]]),axis=0)\n",
        "        print('validation:',[itr,res[0],res[1]*100,validation_fail])\n",
        "        if validation_fail>=validation_fail_limit:\n",
        "            break\n",
        "    \n",
        "    # testing the model\n",
        "    a=best_model.predict(x=test_input)\n",
        "    matrix = confusion_matrix(test_target.argmax(axis=1), a.argmax(axis=1))\n",
        "    cat_acc=np.zeros((np.shape(matrix)[0]+1,))\n",
        "    sum_cat=0\n",
        "    for i in range(np.shape(matrix)[0]):\n",
        "        cat_acc[i]=100*(matrix[i][i]/np.sum(matrix[:,i]))\n",
        "        sum_cat=sum_cat+matrix[i][i]\n",
        "    cat_acc[np.shape(matrix)[0]]=100*sum_cat/np.sum(matrix)\n",
        "    \n",
        "    # accuracy:- green: validation data, blue: training data\n",
        "    plt.plot(log_train[:,0],log_train[:,2],'b--',log_train[:,0],log_validation[:,2],'g')\n",
        "    plt.show()\n",
        "    # loss:- green: validation data, blue: training data\n",
        "    plt.plot(log_train[:,0],log_train[:,1],'b--',log_train[:,0],log_validation[:,1],'g')\n",
        "    plt.show()\n",
        "    # validation checks\n",
        "    plt.plot(log_train[:,0],log_validation[:,3],'g.')\n",
        "    plt.show()\n",
        "    print(matrix)\n",
        "    print(cat_acc)\n",
        "    return model, log_train, log_validation, matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn6WxJQa-vxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "from keras.datasets import mnist\n",
        "#download mnist data and split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train=X_train.reshape(60000,28,28,1)\n",
        "X_test=X_test.reshape(10000,28,28,1)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko6yufdN-4Fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create model\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "#compile model using accuracy to measure model performance\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "xtrain,ytrain,xval,yval=split_data(X_train,y_train,[5,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkXLXNZE_r38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained_model, log_train, log_validation, matrix = lmmaes(model,(xtrain,xval,X_test),(ytrain,yval,y_test),batch=100,itr_limit=19)\n",
        "\n",
        "model.evaluate(x=X_test,y=y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is-2WAMbphUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvRN6gSZpfGH",
        "colab_type": "text"
      },
      "source": [
        "log_validation\n",
        "array([[ 0.        , 12.56754179,  0.05312   ,  0.        ],\n",
        "       [ 1.        ,  4.14396236,  0.7429    ,  0.        ],\n",
        "       [ 2.        ,  3.79903511,  0.7643    ,  0.        ],\n",
        "       [ 3.        ,  2.84162023,  0.8237    ,  0.        ],\n",
        "       [ 4.        ,  2.74491166,  0.8297    ,  0.        ],\n",
        "       [ 5.        ,  2.52892918,  0.8431    ,  0.        ],\n",
        "       [ 6.        ,  2.35968918,  0.8536    ,  0.        ],\n",
        "       [ 7.        ,  2.47251584,  0.8466    ,  1.        ],\n",
        "       [ 8.        ,  2.50475204,  0.8446    ,  2.        ],\n",
        "       [ 9.        ,  2.30327584,  0.8571    ,  0.        ],\n",
        "       [10.        ,  2.39837261,  0.8512    ,  1.        ],\n",
        "       [11.        ,  2.45962137,  0.8474    ,  2.        ],\n",
        "       [12.        ,  2.3129467 ,  0.8565    ,  3.        ],\n",
        "       [13.        ,  2.45156233,  0.8479    ,  4.        ],\n",
        "       [14.        ,  2.33228841,  0.8553    ,  5.        ],\n",
        "       [15.        ,  2.42899699,  0.8493    ,  6.        ]])\n",
        "log_train\n",
        "array([[ 0.        , 12.56754179,  0.05312   ],\n",
        "       [ 1.        ,  4.43473279,  0.72486   ],\n",
        "       [ 2.        ,  4.02855679,  0.75006   ],\n",
        "       [ 3.        ,  3.28325605,  0.7963    ],\n",
        "       [ 4.        ,  3.02149818,  0.81254   ],\n",
        "       [ 5.        ,  2.81293002,  0.82548   ],\n",
        "       [ 6.        ,  2.61532217,  0.83774   ],\n",
        "       [ 7.        ,  2.75361543,  0.82916   ],\n",
        "       [ 8.        ,  2.77811494,  0.82764   ],\n",
        "       [ 9.        ,  2.59436865,  0.83904   ],\n",
        "       [10.        ,  2.65303852,  0.8354    ],\n",
        "       [11.        ,  2.72363577,  0.83102   ],\n",
        "       [12.        ,  2.61242091,  0.83792   ],\n",
        "       [13.        ,  2.71718853,  0.83142   ],\n",
        "       [14.        ,  2.56116537,  0.8411    ],\n",
        "       [15.        ,  2.61790107,  0.83758   ]])\n",
        "matrix\n",
        "array([[ 907,    1,   11,    2,    4,   26,   16,    2,   10,    1],\n",
        "       [   0, 1085,    9,    8,    6,    2,    4,    1,   20,    0],\n",
        "       [  15,   18,  830,   14,   45,    3,   26,   26,   47,    8],\n",
        "       [  25,   13,   46,  822,    4,   29,    1,   13,   40,   17],\n",
        "       [   2,    3,    0,    2,  868,    0,   35,    5,   15,   52],\n",
        "       [  59,   48,    6,   56,   38,  595,   18,    7,   50,   15],\n",
        "       [  28,    6,   12,    7,   31,   11,  837,    5,   20,    1],\n",
        "       [   1,   11,   41,    2,   10,    0,    2,  927,    6,   28],\n",
        "       [   5,   12,   13,   14,    9,   17,    8,   20,  861,   15],\n",
        "       [   8,    4,    7,   17,   81,    9,   12,   48,   25,  798]])\n",
        "category wise accuracy\n",
        "[86.38095238 90.34138218 85.12820513 87.07627119 79.19708029 85.98265896\n",
        " 87.27841502 87.95066414 78.70201097 85.34759358 85.3       ]\n"
      ]
    }
  ]
}