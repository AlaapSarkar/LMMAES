{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LMMAES_evo_paths.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alsr96/LMMAES/blob/master/LMMAES_evo_paths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzZwhinw1APU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from keras import backend as K\n",
        "\n",
        "class structtype():\n",
        "    pass\n",
        "\n",
        "def nw_to_vec(model,layer_idx=None):\n",
        "    n_layers = len(model.layers)\n",
        "    vector=np.empty((0,))\n",
        "    ind=np.zeros((1,))\n",
        "    sum_i=0\n",
        "    if layer_idx==None:\n",
        "        idx=range(n_layers)\n",
        "    else:\n",
        "        idx=layer_idx\n",
        "    for i in idx:\n",
        "        if len(model.layers[i].get_weights())==2:\n",
        "            weights, biases = model.layers[i].get_weights()\n",
        "            s_w=np.size(weights)\n",
        "            sum_i=sum_i+s_w\n",
        "            ind=np.append(ind,sum_i)\n",
        "            w_v=np.reshape(weights,(s_w,))\n",
        "            s_b=np.size(biases)\n",
        "            sum_i=sum_i+s_b\n",
        "            ind=np.append(ind,sum_i)\n",
        "            b_v=np.reshape(biases,(s_b,))\n",
        "            wb=np.append(w_v,b_v)\n",
        "            vector=np.append(vector,wb)\n",
        "    return vector, ind\n",
        "\n",
        "def vec_to_nw(vector,ind,model,layer_idx=None):\n",
        "    n_layers = len(model.layers)\n",
        "    if layer_idx==None:\n",
        "        idx=range(n_layers)\n",
        "    else:\n",
        "        idx=layer_idx\n",
        "    k=0\n",
        "    for i in idx:\n",
        "        if len(model.layers[i].get_weights())==2:\n",
        "            weights,biases=model.layers[i].get_weights()\n",
        "            j1=k\n",
        "            j2=k+1\n",
        "            j3=k+2\n",
        "            weights=np.reshape(vector[int(ind[j1]):int(ind[j2])],np.shape(weights))\n",
        "            biases=np.reshape(vector[int(ind[j2]):int(ind[j3])],np.shape(biases))\n",
        "            model.layers[i].set_weights((weights,biases))\n",
        "            k=k+2\n",
        "    return model\n",
        "\n",
        "#split=[15,15]\n",
        "def split_data(input_data,target2,split):\n",
        "    n_samples=len(target2)\n",
        "    s1_input=input_data[0:int(split[0]*n_samples/(split[0]+split[1]))]\n",
        "    s1_target=target2[0:int(split[0]*n_samples/(split[0]+split[1]))]\n",
        "    s2_input=input_data[int(split[0]*n_samples/(split[0]+split[1])):n_samples]\n",
        "    s2_target=target2[int(split[0]*n_samples/(split[0]+split[1])):n_samples]\n",
        "    return s1_input, s1_target, s2_input, s2_target\n",
        "\n",
        "# obtain gradients\n",
        "def get_gradients(model,inputv,output):\n",
        "  grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
        "  symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
        "  f = K.function(symb_inputs, grads)\n",
        "  x, y, sample_weight = model._standardize_user_data(inputv, output)\n",
        "  output_grad = f(x + y + sample_weight)\n",
        "  return output_grad\n",
        "\n",
        "class lmmaes(object):\n",
        "    def __init__(self,model,n_candidates = None,mu = None, m = None,sigma=1/10,function_budget=10000,function_target=None,layer_idx=None):\n",
        "      self.sigma=sigma\n",
        "      # converting the weights and biases to a row vector\n",
        "      self.layer_idx=layer_idx\n",
        "      self.y, self.ind = nw_to_vec(model,layer_idx=self.layer_idx)\n",
        "      \n",
        "      # number of layers to optimise\n",
        "      if self.layer_idx==None:\n",
        "          self.n_layers = len(model.layers)\n",
        "          itr = range(self.n_layers)\n",
        "      else:\n",
        "          self.n_layers = len(self.layer_idx)\n",
        "          itr = self.layer_idx\n",
        "\n",
        "      # calculating number of dimensions\n",
        "      self.n_dimensions=0\n",
        "      for i in itr:\n",
        "          if len(model.layers[i].get_weights())==2:\n",
        "              weights, biases = model.layers[i].get_weights()\n",
        "              self.n_dimensions=np.size(weights)+np.size(biases)+self.n_dimensions\n",
        "\n",
        "      # number of candidate solutions generated\n",
        "      self.n_candidates=n_candidates\n",
        "      if self.n_candidates==None:\n",
        "          self.n_candidates = 2*int((4 + np.floor(3*np.log(self.n_dimensions)))/2)\n",
        "\n",
        "      if mu==None:\n",
        "        # number of best solutions selected\n",
        "        self.mu = int(self.n_candidates/2)\n",
        "      else:\n",
        "        self.mu=mu\n",
        "\n",
        "      # weights for selected solutions\n",
        "      self.w = np.empty([0,0])\n",
        "      for i in range(int(self.mu)):\n",
        "          self.w = np.append(self.w,np.log(self.mu+0.5)-np.log(i+1))\n",
        "      sum_w = np.sum(self.w)\n",
        "      self.w = self.w/sum_w\n",
        "\n",
        "\n",
        "      self.mu_w = 1/(np.sum(np.square(self.w)))\n",
        "\n",
        "      if m==None:\n",
        "        # number of evolution paths\n",
        "        self.m = self.n_candidates\n",
        "      else:\n",
        "        self.m=m\n",
        "\n",
        "      self.c_sigma = 2*self.n_candidates/self.n_dimensions\n",
        "      self.const1=np.sqrt(self.mu_w*self.c_sigma*(2-self.c_sigma))\n",
        "\n",
        "      # learning rates\n",
        "      self.c_d = np.empty([0,0])\n",
        "      self.c_c = np.empty([0,0])\n",
        "      self.const2=np.empty([0,0])\n",
        "      for i in range(int(self.m)):\n",
        "          self.c_d = np.append(self.c_d,1/(self.n_dimensions*(1.5**i)))\n",
        "          self.c_c = np.append(self.c_c,self.n_candidates/(self.n_dimensions*(4**i)))\n",
        "          self.const2=np.append(self.const2,np.sqrt(self.mu_w*self.c_c[i]*(2-self.c_c[i])))\n",
        "      #print(self.c_c,self.c_d,self.const1,self.const2)\n",
        "      self.t=0\n",
        "\n",
        "      # length of evolution paths (exponentially fading record of recent most successful steps)\n",
        "      self.p_sigma = np.zeros((self.n_dimensions,))\n",
        "\n",
        "      # vectors modelling deviation of transformation matrix from identity matrix\n",
        "      self.m_i = np.zeros((int(self.m), self.n_dimensions))\n",
        "    \n",
        "    # 1 step of optimisation\n",
        "    def train_on_batch(self,model,train_data,divide_data=False,use_gradients=False):\n",
        "      self.use_gradients=use_gradients\n",
        "      self.func_calls=0\n",
        "      train_input=train_data[0]\n",
        "      train_target=train_data[1]\n",
        "      # sampling a normal distribution\n",
        "      samples=np.random.randn(int(self.n_candidates/2),self.n_dimensions)\n",
        "      z=np.zeros((self.n_candidates,self.n_dimensions))\n",
        "      for i in range(int(self.n_candidates/2)):\n",
        "        z[2*i]=samples[i]\n",
        "        z[2*i+1]=-1*samples[i]\n",
        "      d = np.copy(z)\n",
        "      #print(np.sum(d))\n",
        "      f_list=np.empty((int(self.n_candidates),1))\n",
        "\n",
        "      for j in range(min(self.t, self.m)):\n",
        "            d = ((1 - self.c_d[j])*d) + (self.c_d[j] *np.outer(np.dot(d, self.m_i[j, :]), self.m_i[j, :]))\n",
        "      \n",
        "      for i in range(int(self.n_candidates)):\n",
        "          model=vec_to_nw(self.y+self.sigma*d[i],self.ind,model,layer_idx=self.layer_idx)\n",
        "          # evaluating the loss\n",
        "          num_samples=len(train_target)-1\n",
        "          if divide_data==False:\n",
        "            res=model.evaluate(x=train_input,y=train_target,verbose=0)\n",
        "          else:\n",
        "            res=model.evaluate(x=train_input[int(i*num_samples/self.n_candidates):int((i+1)*num_samples/self.n_candidates)],y=train_target[int(i*num_samples/self.n_candidates):int((i+1)*num_samples/self.n_candidates)],verbose=0)\n",
        "          self.func_calls=self.func_calls+1\n",
        "          f_list[i][0] = res[0]\n",
        "      \n",
        "\n",
        "      # sorting the solutions based on fitness\n",
        "      sortidx_f = f_list.argsort(axis=0)\n",
        "      # selecting the best 'mu' out of the 'lambda' mutations\n",
        "      sortidx_f = sortidx_f[0:int(self.mu)]\n",
        "      best_list = np.empty([int(self.mu),self.n_dimensions])\n",
        "      j = 0\n",
        "      # weighted average of the best mutations\n",
        "      for i in sortidx_f:\n",
        "          best_list[j] = self.w[j]*d[i]\n",
        "          j = j+1\n",
        "      \n",
        "      y_update = self.sigma*np.sum(best_list,0)\n",
        "      y_next = self.y + y_update\n",
        "\n",
        "      self.y = y_next\n",
        "      model=vec_to_nw(self.y,self.ind,model,layer_idx=self.layer_idx)\n",
        "      \n",
        "      best_list2 = np.empty([int(self.mu),self.n_dimensions])\n",
        "      j = 0\n",
        "      for i in sortidx_f:\n",
        "          best_list2[j] = self.w[j]*z[i]\n",
        "          j = j+1\n",
        "      weighted_sum_norm=np.sum(best_list2,0)\n",
        "      p_sigma_next = (1-self.c_sigma)*self.p_sigma + self.const1*weighted_sum_norm\n",
        "      mag_p_sigma_next = np.linalg.norm(p_sigma_next)\n",
        "      \n",
        "      m_update=weighted_sum_norm\n",
        "\n",
        "      if self.use_gradients==True:\n",
        "        print('lololol')\n",
        "        gradients=get_gradients(model,train_input,train_target)\n",
        "        gradient_vector=np.empty((0,0))\n",
        "        for i in range(len(gradients)):\n",
        "          gradient_vector=np.append(gradient_vector,gradients[i].reshape(np.size(gradients[i]),))\n",
        "        gradient_vector=np.linalg.norm(weighted_sum_norm)*(-1*gradient_vector/np.linalg.norm(gradient_vector))\n",
        "        m_update=gradient_vector\n",
        "\n",
        "      # M update\n",
        "      for i in range(int(self.m)):\n",
        "          self.m_i[i] = (1-self.c_c[i])*self.m_i[i] + self.const2[i]*m_update\n",
        "      sigma_next = self.sigma*np.exp(self.c_sigma*(((mag_p_sigma_next**2)/self.n_dimensions)-1)/2)\n",
        "      self.t=self.t+1\n",
        "      self.sigma = sigma_next\n",
        "      self.p_sigma = p_sigma_next\n",
        "      jhlk=sortidx_f[0][0]\n",
        "      #print(f_list[int(jhlk)],self.t,self.sigma)\n",
        "      return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbW11aEj2ORF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "from keras.datasets import mnist\n",
        "#download mnist data and split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train=X_train.reshape(60000,28,28,1)\n",
        "X_test=X_test.reshape(10000,28,28,1)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "638GI_Lr2Hjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create model\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "#compile model using accuracy to measure model performance\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "xtrain,ytrain,xval,yval=split_data(X_train,y_train,[5,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97lkRphBD5-W",
        "colab_type": "code",
        "outputId": "f9cd6b6d-947f-4fa1-a809-0ef336ec086d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "lm=lmmaes(model,n_candidates = 40, mu=15, m=40, sigma=1/100,layer_idx=None)\n",
        "\n",
        "batch=int(1000)\n",
        "no_of_epochs=5\n",
        "print('Number of dimensions: %g; lambda: %g; mu: %g; m: %g; batch size: %g, no. of epochs: %g'%(lm.n_dimensions,lm.n_candidates,lm.mu,lm.m,batch,no_of_epochs))\n",
        "run_settings=lm.n_dimensions,lm.n_candidates,lm.mu,lm.m,lm.sigma,batch,no_of_epochs\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "res_current_best=model.evaluate(x=xval,y=yval)\n",
        "y_current_best,ind=nw_to_vec(model,layer_idx=None)\n",
        "validation_fail=0\n",
        "log_train=np.array([-1,0,lm.sigma,res_current_best[0],res_current_best[1]])\n",
        "log_validation=np.array([-1,res_current_best[0],100*res_current_best[1]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of dimensions: 203434; lambda: 40; mu: 15; m: 40; batch size: 1000, no. of epochs: 5\n",
            "10000/10000 [==============================] - 1s 108us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL3LRDUAZEw7",
        "colab_type": "code",
        "outputId": "b4b7062f-b497-4c50-a86d-b48049918b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(no_of_epochs):\n",
        "  # current best solution\n",
        "  #xtrain,ytrain=unison_shuffled_copies(xtrain,ytrain)\n",
        "  #print(batch/lm.n_candidates)\n",
        "  no_samples=len(ytrain)-1\n",
        "  n_groups=int(no_samples/batch)\n",
        "  \n",
        "  for j in range(n_groups):\n",
        "    # 1 step of optimisation\n",
        "    model = lm.train_on_batch(model,train_data=(xtrain[int(j*no_samples/n_groups):int((j+1)*no_samples/n_groups)],ytrain[int(j*no_samples/n_groups):int((j+1)*no_samples/n_groups)]),divide_data=False)\n",
        "    res_new=model.evaluate(x=xtrain[int(j*no_samples/n_groups):int((j+1)*no_samples/n_groups)],y=ytrain[int(j*no_samples/n_groups):int((j+1)*no_samples/n_groups)],verbose=0)\n",
        "    log_train=np.append(log_train,[i,j,lm.sigma,res_new[0],res_new[1]],axis=0)\n",
        "    print('epoch: %g;batch: %g; sigma: %g; loss: %.2f; accuracy: %.2f' % (i,j,lm.sigma,res_new[0],100*res_new[1]))\n",
        "  # validation\n",
        "  res_val=model.evaluate(x=xval,y=yval)\n",
        "  log_validation=np.append(log_validation,[i,res_val[0],100*res_val[1]],axis=0)\n",
        "  if res_new[1]>res_current_best[1]:\n",
        "    res_current_best=res_new\n",
        "    #y_current_best=y_new\n",
        "    validation_fail=0\n",
        "    y_current_best,ind=nw_to_vec(model)\n",
        "  else:\n",
        "    #model=vec_to_nw(y_current_best,ind,model,layer_idx=None)\n",
        "    validation_fail=validation_fail+1\n",
        "  \n",
        "  if validation_fail>=6:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0;batch: 0; sigma: 0.00999804; loss: 11.45; accuracy: 17.84\n",
            "epoch: 0;batch: 1; sigma: 0.00999607; loss: 10.48; accuracy: 24.90\n",
            "epoch: 0;batch: 2; sigma: 0.00999411; loss: 10.63; accuracy: 25.37\n",
            "epoch: 0;batch: 3; sigma: 0.00999215; loss: 10.47; accuracy: 27.35\n",
            "epoch: 0;batch: 4; sigma: 0.0099902; loss: 10.11; accuracy: 29.71\n",
            "epoch: 0;batch: 5; sigma: 0.00998824; loss: 10.05; accuracy: 30.26\n",
            "epoch: 0;batch: 6; sigma: 0.00998629; loss: 10.09; accuracy: 29.71\n",
            "epoch: 0;batch: 7; sigma: 0.00998434; loss: 9.11; accuracy: 31.54\n",
            "epoch: 0;batch: 8; sigma: 0.00998239; loss: 9.38; accuracy: 31.86\n",
            "epoch: 0;batch: 9; sigma: 0.00998044; loss: 8.09; accuracy: 38.43\n",
            "epoch: 0;batch: 10; sigma: 0.00997849; loss: 8.28; accuracy: 39.57\n",
            "epoch: 0;batch: 11; sigma: 0.00997655; loss: 7.95; accuracy: 41.76\n",
            "epoch: 0;batch: 12; sigma: 0.0099746; loss: 7.82; accuracy: 42.02\n",
            "epoch: 0;batch: 13; sigma: 0.00997266; loss: 7.44; accuracy: 44.61\n",
            "epoch: 0;batch: 14; sigma: 0.00997072; loss: 7.42; accuracy: 45.29\n",
            "epoch: 0;batch: 15; sigma: 0.00996878; loss: 7.00; accuracy: 47.99\n",
            "epoch: 0;batch: 16; sigma: 0.00996685; loss: 6.54; accuracy: 51.18\n",
            "epoch: 0;batch: 17; sigma: 0.00996491; loss: 6.94; accuracy: 48.73\n",
            "epoch: 0;batch: 18; sigma: 0.00996298; loss: 6.54; accuracy: 50.64\n",
            "epoch: 0;batch: 19; sigma: 0.00996105; loss: 6.49; accuracy: 52.25\n",
            "epoch: 0;batch: 20; sigma: 0.00995912; loss: 5.95; accuracy: 56.42\n",
            "epoch: 0;batch: 21; sigma: 0.00995719; loss: 6.77; accuracy: 50.59\n",
            "epoch: 0;batch: 22; sigma: 0.00995526; loss: 6.39; accuracy: 53.24\n",
            "epoch: 0;batch: 23; sigma: 0.00995334; loss: 6.18; accuracy: 54.95\n",
            "epoch: 0;batch: 24; sigma: 0.00995142; loss: 5.79; accuracy: 54.71\n",
            "epoch: 0;batch: 25; sigma: 0.0099495; loss: 5.49; accuracy: 57.20\n",
            "epoch: 0;batch: 26; sigma: 0.00994758; loss: 6.06; accuracy: 53.82\n",
            "epoch: 0;batch: 27; sigma: 0.00994566; loss: 5.17; accuracy: 60.10\n",
            "epoch: 0;batch: 28; sigma: 0.00994374; loss: 5.41; accuracy: 61.51\n",
            "epoch: 0;batch: 29; sigma: 0.00994183; loss: 6.46; accuracy: 52.55\n",
            "epoch: 0;batch: 30; sigma: 0.00993991; loss: 5.82; accuracy: 56.81\n",
            "epoch: 0;batch: 31; sigma: 0.009938; loss: 6.11; accuracy: 55.88\n",
            "epoch: 0;batch: 32; sigma: 0.00993609; loss: 4.88; accuracy: 63.24\n",
            "epoch: 0;batch: 33; sigma: 0.00993418; loss: 4.52; accuracy: 66.01\n",
            "epoch: 0;batch: 34; sigma: 0.00993228; loss: 5.27; accuracy: 61.37\n",
            "epoch: 0;batch: 35; sigma: 0.00993037; loss: 4.95; accuracy: 63.43\n",
            "epoch: 0;batch: 36; sigma: 0.00992847; loss: 5.26; accuracy: 62.19\n",
            "epoch: 0;batch: 37; sigma: 0.00992657; loss: 4.81; accuracy: 63.53\n",
            "epoch: 0;batch: 38; sigma: 0.00992467; loss: 4.83; accuracy: 63.96\n",
            "epoch: 0;batch: 39; sigma: 0.00992277; loss: 5.04; accuracy: 63.04\n",
            "epoch: 0;batch: 40; sigma: 0.00992087; loss: 5.15; accuracy: 60.78\n",
            "epoch: 0;batch: 41; sigma: 0.00991897; loss: 4.84; accuracy: 64.94\n",
            "epoch: 0;batch: 42; sigma: 0.00991708; loss: 4.85; accuracy: 63.73\n",
            "epoch: 0;batch: 43; sigma: 0.00991519; loss: 5.32; accuracy: 62.19\n",
            "epoch: 0;batch: 44; sigma: 0.0099133; loss: 5.10; accuracy: 63.04\n",
            "epoch: 0;batch: 45; sigma: 0.00991141; loss: 4.92; accuracy: 63.53\n",
            "epoch: 0;batch: 46; sigma: 0.00990952; loss: 4.97; accuracy: 63.96\n",
            "epoch: 0;batch: 47; sigma: 0.00990763; loss: 4.43; accuracy: 68.43\n",
            "epoch: 0;batch: 48; sigma: 0.00990575; loss: 5.22; accuracy: 62.39\n",
            "10000/10000 [==============================] - 1s 101us/step\n",
            "epoch: 1;batch: 0; sigma: 0.00990386; loss: 4.68; accuracy: 66.08\n",
            "epoch: 1;batch: 1; sigma: 0.00990198; loss: 4.59; accuracy: 66.96\n",
            "epoch: 1;batch: 2; sigma: 0.0099001; loss: 3.98; accuracy: 71.11\n",
            "epoch: 1;batch: 3; sigma: 0.00989822; loss: 4.54; accuracy: 67.06\n",
            "epoch: 1;batch: 4; sigma: 0.00989634; loss: 4.18; accuracy: 69.22\n",
            "epoch: 1;batch: 5; sigma: 0.00989447; loss: 4.01; accuracy: 71.20\n",
            "epoch: 1;batch: 6; sigma: 0.00989259; loss: 4.02; accuracy: 70.59\n",
            "epoch: 1;batch: 7; sigma: 0.00989072; loss: 4.50; accuracy: 67.48\n",
            "epoch: 1;batch: 8; sigma: 0.00988885; loss: 4.45; accuracy: 68.73\n",
            "epoch: 1;batch: 9; sigma: 0.00988698; loss: 3.78; accuracy: 73.14\n",
            "epoch: 1;batch: 10; sigma: 0.00988511; loss: 3.88; accuracy: 71.50\n",
            "epoch: 1;batch: 11; sigma: 0.00988325; loss: 4.18; accuracy: 70.20\n",
            "epoch: 1;batch: 12; sigma: 0.00988138; loss: 4.64; accuracy: 66.80\n",
            "epoch: 1;batch: 13; sigma: 0.00987952; loss: 4.27; accuracy: 69.51\n",
            "epoch: 1;batch: 14; sigma: 0.00987766; loss: 4.92; accuracy: 66.47\n",
            "epoch: 1;batch: 15; sigma: 0.0098758; loss: 4.63; accuracy: 67.19\n",
            "epoch: 1;batch: 16; sigma: 0.00987395; loss: 4.67; accuracy: 67.45\n",
            "epoch: 1;batch: 17; sigma: 0.00987209; loss: 4.21; accuracy: 70.49\n",
            "epoch: 1;batch: 18; sigma: 0.00987024; loss: 4.10; accuracy: 71.01\n",
            "epoch: 1;batch: 19; sigma: 0.00986838; loss: 3.93; accuracy: 71.76\n",
            "epoch: 1;batch: 20; sigma: 0.00986653; loss: 4.04; accuracy: 71.60\n",
            "epoch: 1;batch: 21; sigma: 0.00986468; loss: 3.65; accuracy: 73.82\n",
            "epoch: 1;batch: 22; sigma: 0.00986284; loss: 4.07; accuracy: 71.67\n",
            "epoch: 1;batch: 23; sigma: 0.00986099; loss: 3.93; accuracy: 72.67\n",
            "epoch: 1;batch: 24; sigma: 0.00985914; loss: 4.09; accuracy: 71.47\n",
            "epoch: 1;batch: 25; sigma: 0.0098573; loss: 3.48; accuracy: 75.81\n",
            "epoch: 1;batch: 26; sigma: 0.00985546; loss: 3.98; accuracy: 72.06\n",
            "epoch: 1;batch: 27; sigma: 0.00985362; loss: 3.58; accuracy: 75.39\n",
            "epoch: 1;batch: 28; sigma: 0.00985178; loss: 4.00; accuracy: 72.09\n",
            "epoch: 1;batch: 29; sigma: 0.00984994; loss: 4.39; accuracy: 69.61\n",
            "epoch: 1;batch: 30; sigma: 0.00984811; loss: 4.16; accuracy: 70.81\n",
            "epoch: 1;batch: 31; sigma: 0.00984627; loss: 4.58; accuracy: 68.73\n",
            "epoch: 1;batch: 32; sigma: 0.00984444; loss: 3.64; accuracy: 74.51\n",
            "epoch: 1;batch: 33; sigma: 0.00984261; loss: 3.34; accuracy: 77.08\n",
            "epoch: 1;batch: 34; sigma: 0.00984078; loss: 4.02; accuracy: 72.16\n",
            "epoch: 1;batch: 35; sigma: 0.00983895; loss: 3.75; accuracy: 73.92\n",
            "epoch: 1;batch: 36; sigma: 0.00983713; loss: 4.13; accuracy: 71.89\n",
            "epoch: 1;batch: 37; sigma: 0.0098353; loss: 3.47; accuracy: 75.78\n",
            "epoch: 1;batch: 38; sigma: 0.00983348; loss: 3.66; accuracy: 75.02\n",
            "epoch: 1;batch: 39; sigma: 0.00983165; loss: 3.75; accuracy: 73.92\n",
            "epoch: 1;batch: 40; sigma: 0.00982983; loss: 3.70; accuracy: 74.31\n",
            "epoch: 1;batch: 41; sigma: 0.00982801; loss: 4.12; accuracy: 71.40\n",
            "epoch: 1;batch: 42; sigma: 0.0098262; loss: 3.67; accuracy: 74.31\n",
            "epoch: 1;batch: 43; sigma: 0.00982438; loss: 3.96; accuracy: 72.18\n",
            "epoch: 1;batch: 44; sigma: 0.00982256; loss: 3.80; accuracy: 73.92\n",
            "epoch: 1;batch: 45; sigma: 0.00982075; loss: 3.68; accuracy: 73.43\n",
            "epoch: 1;batch: 46; sigma: 0.00981894; loss: 3.84; accuracy: 73.46\n",
            "epoch: 1;batch: 47; sigma: 0.00981713; loss: 3.34; accuracy: 77.06\n",
            "epoch: 1;batch: 48; sigma: 0.00981532; loss: 4.16; accuracy: 70.81\n",
            "10000/10000 [==============================] - 1s 101us/step\n",
            "epoch: 2;batch: 0; sigma: 0.00981351; loss: 3.95; accuracy: 73.63\n",
            "epoch: 2;batch: 1; sigma: 0.0098117; loss: 3.60; accuracy: 75.20\n",
            "epoch: 2;batch: 2; sigma: 0.0098099; loss: 3.25; accuracy: 76.98\n",
            "epoch: 2;batch: 3; sigma: 0.0098081; loss: 3.54; accuracy: 76.18\n",
            "epoch: 2;batch: 4; sigma: 0.00980629; loss: 3.25; accuracy: 77.94\n",
            "epoch: 2;batch: 5; sigma: 0.00980449; loss: 3.19; accuracy: 77.47\n",
            "epoch: 2;batch: 6; sigma: 0.00980269; loss: 3.30; accuracy: 76.76\n",
            "epoch: 2;batch: 7; sigma: 0.00980089; loss: 3.64; accuracy: 75.12\n",
            "epoch: 2;batch: 8; sigma: 0.00979909; loss: 3.94; accuracy: 73.33\n",
            "epoch: 2;batch: 9; sigma: 0.0097973; loss: 3.12; accuracy: 78.33\n",
            "epoch: 2;batch: 10; sigma: 0.0097955; loss: 3.13; accuracy: 77.28\n",
            "epoch: 2;batch: 11; sigma: 0.00979371; loss: 3.87; accuracy: 73.24\n",
            "epoch: 2;batch: 12; sigma: 0.00979192; loss: 3.84; accuracy: 74.34\n",
            "epoch: 2;batch: 13; sigma: 0.00979013; loss: 3.77; accuracy: 74.61\n",
            "epoch: 2;batch: 14; sigma: 0.00978834; loss: 4.20; accuracy: 72.06\n",
            "epoch: 2;batch: 15; sigma: 0.00978655; loss: 3.84; accuracy: 73.95\n",
            "epoch: 2;batch: 16; sigma: 0.00978476; loss: 3.97; accuracy: 72.75\n",
            "epoch: 2;batch: 17; sigma: 0.00978297; loss: 3.65; accuracy: 74.80\n",
            "epoch: 2;batch: 18; sigma: 0.00978119; loss: 3.38; accuracy: 77.08\n",
            "epoch: 2;batch: 19; sigma: 0.00977941; loss: 3.53; accuracy: 75.69\n",
            "epoch: 2;batch: 20; sigma: 0.00977763; loss: 3.48; accuracy: 76.20\n",
            "epoch: 2;batch: 21; sigma: 0.00977585; loss: 3.12; accuracy: 78.43\n",
            "epoch: 2;batch: 22; sigma: 0.00977407; loss: 3.67; accuracy: 75.00\n",
            "epoch: 2;batch: 23; sigma: 0.00977229; loss: 3.56; accuracy: 76.30\n",
            "epoch: 2;batch: 24; sigma: 0.00977052; loss: 3.65; accuracy: 75.00\n",
            "epoch: 2;batch: 25; sigma: 0.00976874; loss: 3.36; accuracy: 77.18\n",
            "epoch: 2;batch: 26; sigma: 0.00976697; loss: 3.79; accuracy: 74.51\n",
            "epoch: 2;batch: 27; sigma: 0.0097652; loss: 3.15; accuracy: 78.63\n",
            "epoch: 2;batch: 28; sigma: 0.00976343; loss: 3.73; accuracy: 75.42\n",
            "epoch: 2;batch: 29; sigma: 0.00976166; loss: 3.88; accuracy: 73.53\n",
            "epoch: 2;batch: 30; sigma: 0.00975989; loss: 3.70; accuracy: 74.73\n",
            "epoch: 2;batch: 31; sigma: 0.00975813; loss: 4.14; accuracy: 71.96\n",
            "epoch: 2;batch: 32; sigma: 0.00975636; loss: 3.43; accuracy: 77.25\n",
            "epoch: 2;batch: 33; sigma: 0.0097546; loss: 3.04; accuracy: 79.24\n",
            "epoch: 2;batch: 34; sigma: 0.00975284; loss: 3.69; accuracy: 75.10\n",
            "epoch: 2;batch: 35; sigma: 0.00975108; loss: 3.20; accuracy: 78.24\n",
            "epoch: 2;batch: 36; sigma: 0.00974932; loss: 3.93; accuracy: 74.24\n",
            "epoch: 2;batch: 37; sigma: 0.00974756; loss: 3.45; accuracy: 76.67\n",
            "epoch: 2;batch: 38; sigma: 0.00974581; loss: 3.52; accuracy: 76.40\n",
            "epoch: 2;batch: 39; sigma: 0.00974405; loss: 3.57; accuracy: 76.18\n",
            "epoch: 2;batch: 40; sigma: 0.0097423; loss: 3.62; accuracy: 75.78\n",
            "epoch: 2;batch: 41; sigma: 0.00974055; loss: 3.76; accuracy: 74.73\n",
            "epoch: 2;batch: 42; sigma: 0.0097388; loss: 3.55; accuracy: 76.08\n",
            "epoch: 2;batch: 43; sigma: 0.00973705; loss: 3.71; accuracy: 74.83\n",
            "epoch: 2;batch: 44; sigma: 0.0097353; loss: 3.63; accuracy: 76.08\n",
            "epoch: 2;batch: 45; sigma: 0.00973355; loss: 3.78; accuracy: 73.82\n",
            "epoch: 2;batch: 46; sigma: 0.00973181; loss: 3.48; accuracy: 76.69\n",
            "epoch: 2;batch: 47; sigma: 0.00973006; loss: 3.56; accuracy: 76.08\n",
            "epoch: 2;batch: 48; sigma: 0.00972832; loss: 3.90; accuracy: 72.97\n",
            "10000/10000 [==============================] - 1s 101us/step\n",
            "epoch: 3;batch: 0; sigma: 0.00972658; loss: 3.46; accuracy: 76.27\n",
            "epoch: 3;batch: 1; sigma: 0.00972484; loss: 3.35; accuracy: 77.06\n",
            "epoch: 3;batch: 2; sigma: 0.0097231; loss: 3.12; accuracy: 79.04\n",
            "epoch: 3;batch: 3; sigma: 0.00972136; loss: 3.55; accuracy: 75.78\n",
            "epoch: 3;batch: 4; sigma: 0.00971963; loss: 3.25; accuracy: 78.24\n",
            "epoch: 3;batch: 5; sigma: 0.00971789; loss: 3.02; accuracy: 79.53\n",
            "epoch: 3;batch: 6; sigma: 0.00971616; loss: 3.10; accuracy: 79.41\n",
            "epoch: 3;batch: 7; sigma: 0.00971443; loss: 3.45; accuracy: 76.49\n",
            "epoch: 3;batch: 8; sigma: 0.0097127; loss: 3.49; accuracy: 76.67\n",
            "epoch: 3;batch: 9; sigma: 0.00971097; loss: 3.09; accuracy: 79.61\n",
            "epoch: 3;batch: 10; sigma: 0.00970924; loss: 3.10; accuracy: 79.14\n",
            "epoch: 3;batch: 11; sigma: 0.00970751; loss: 3.68; accuracy: 75.10\n",
            "epoch: 3;batch: 12; sigma: 0.00970579; loss: 3.58; accuracy: 76.30\n",
            "epoch: 3;batch: 13; sigma: 0.00970406; loss: 3.65; accuracy: 75.88\n",
            "epoch: 3;batch: 14; sigma: 0.00970234; loss: 4.05; accuracy: 72.35\n",
            "epoch: 3;batch: 15; sigma: 0.00970062; loss: 3.74; accuracy: 74.63\n",
            "epoch: 3;batch: 16; sigma: 0.0096989; loss: 3.71; accuracy: 74.80\n",
            "epoch: 3;batch: 17; sigma: 0.00969718; loss: 3.33; accuracy: 77.45\n",
            "epoch: 3;batch: 18; sigma: 0.00969547; loss: 3.24; accuracy: 78.06\n",
            "epoch: 3;batch: 19; sigma: 0.00969375; loss: 3.44; accuracy: 77.06\n",
            "epoch: 3;batch: 20; sigma: 0.00969204; loss: 3.37; accuracy: 77.86\n",
            "epoch: 3;batch: 21; sigma: 0.00969033; loss: 2.99; accuracy: 79.80\n",
            "epoch: 3;batch: 22; sigma: 0.00968862; loss: 3.58; accuracy: 75.88\n",
            "epoch: 3;batch: 23; sigma: 0.00968691; loss: 3.25; accuracy: 77.86\n",
            "epoch: 3;batch: 24; sigma: 0.00968521; loss: 3.70; accuracy: 76.27\n",
            "epoch: 3;batch: 25; sigma: 0.0096835; loss: 3.20; accuracy: 78.16\n",
            "epoch: 3;batch: 26; sigma: 0.0096818; loss: 3.59; accuracy: 76.18\n",
            "epoch: 3;batch: 27; sigma: 0.0096801; loss: 2.98; accuracy: 80.49\n",
            "epoch: 3;batch: 28; sigma: 0.0096784; loss: 3.54; accuracy: 76.59\n",
            "epoch: 3;batch: 29; sigma: 0.0096767; loss: 3.49; accuracy: 75.69\n",
            "epoch: 3;batch: 30; sigma: 0.009675; loss: 3.52; accuracy: 77.18\n",
            "epoch: 3;batch: 31; sigma: 0.0096733; loss: 3.85; accuracy: 75.29\n",
            "epoch: 3;batch: 32; sigma: 0.00967161; loss: 3.19; accuracy: 78.82\n",
            "epoch: 3;batch: 33; sigma: 0.00966992; loss: 2.87; accuracy: 81.19\n",
            "epoch: 3;batch: 34; sigma: 0.00966822; loss: 3.34; accuracy: 77.94\n",
            "epoch: 3;batch: 35; sigma: 0.00966653; loss: 2.84; accuracy: 81.08\n",
            "epoch: 3;batch: 36; sigma: 0.00966484; loss: 3.78; accuracy: 75.42\n",
            "epoch: 3;batch: 37; sigma: 0.00966316; loss: 3.07; accuracy: 79.51\n",
            "epoch: 3;batch: 38; sigma: 0.00966147; loss: 3.26; accuracy: 78.35\n",
            "epoch: 3;batch: 39; sigma: 0.00965978; loss: 3.26; accuracy: 78.04\n",
            "epoch: 3;batch: 40; sigma: 0.0096581; loss: 3.39; accuracy: 77.35\n",
            "epoch: 3;batch: 41; sigma: 0.00965642; loss: 3.71; accuracy: 75.51\n",
            "epoch: 3;batch: 42; sigma: 0.00965473; loss: 3.30; accuracy: 78.04\n",
            "epoch: 3;batch: 43; sigma: 0.00965305; loss: 3.19; accuracy: 78.75\n",
            "epoch: 3;batch: 44; sigma: 0.00965138; loss: 3.48; accuracy: 77.65\n",
            "epoch: 3;batch: 45; sigma: 0.0096497; loss: 3.53; accuracy: 77.06\n",
            "epoch: 3;batch: 46; sigma: 0.00964802; loss: 3.33; accuracy: 78.45\n",
            "epoch: 3;batch: 47; sigma: 0.00964635; loss: 3.27; accuracy: 78.33\n",
            "epoch: 3;batch: 48; sigma: 0.00964468; loss: 3.32; accuracy: 78.26\n",
            "10000/10000 [==============================] - 1s 100us/step\n",
            "epoch: 4;batch: 0; sigma: 0.009643; loss: 3.14; accuracy: 79.51\n",
            "epoch: 4;batch: 1; sigma: 0.00964133; loss: 3.29; accuracy: 78.73\n",
            "epoch: 4;batch: 2; sigma: 0.00963966; loss: 2.76; accuracy: 81.88\n",
            "epoch: 4;batch: 3; sigma: 0.009638; loss: 3.15; accuracy: 80.00\n",
            "epoch: 4;batch: 4; sigma: 0.00963633; loss: 3.06; accuracy: 80.10\n",
            "epoch: 4;batch: 5; sigma: 0.00963466; loss: 2.89; accuracy: 81.10\n",
            "epoch: 4;batch: 6; sigma: 0.009633; loss: 3.13; accuracy: 79.41\n",
            "epoch: 4;batch: 7; sigma: 0.00963134; loss: 3.25; accuracy: 78.35\n",
            "epoch: 4;batch: 8; sigma: 0.00962967; loss: 3.44; accuracy: 77.06\n",
            "epoch: 4;batch: 9; sigma: 0.00962801; loss: 2.92; accuracy: 80.20\n",
            "epoch: 4;batch: 10; sigma: 0.00962636; loss: 2.74; accuracy: 81.98\n",
            "epoch: 4;batch: 11; sigma: 0.0096247; loss: 3.52; accuracy: 76.76\n",
            "epoch: 4;batch: 12; sigma: 0.00962304; loss: 3.50; accuracy: 76.98\n",
            "epoch: 4;batch: 13; sigma: 0.00962139; loss: 3.30; accuracy: 78.73\n",
            "epoch: 4;batch: 14; sigma: 0.00961973; loss: 3.63; accuracy: 75.69\n",
            "epoch: 4;batch: 15; sigma: 0.00961808; loss: 3.26; accuracy: 78.65\n",
            "epoch: 4;batch: 16; sigma: 0.00961643; loss: 3.51; accuracy: 77.16\n",
            "epoch: 4;batch: 17; sigma: 0.00961478; loss: 3.09; accuracy: 80.10\n",
            "epoch: 4;batch: 18; sigma: 0.00961313; loss: 3.02; accuracy: 80.02\n",
            "epoch: 4;batch: 19; sigma: 0.00961148; loss: 3.07; accuracy: 80.10\n",
            "epoch: 4;batch: 20; sigma: 0.00960984; loss: 3.22; accuracy: 79.04\n",
            "epoch: 4;batch: 21; sigma: 0.00960819; loss: 2.88; accuracy: 80.78\n",
            "epoch: 4;batch: 22; sigma: 0.00960655; loss: 3.32; accuracy: 78.63\n",
            "epoch: 4;batch: 23; sigma: 0.00960491; loss: 3.02; accuracy: 80.12\n",
            "epoch: 4;batch: 24; sigma: 0.00960326; loss: 3.23; accuracy: 79.22\n",
            "epoch: 4;batch: 25; sigma: 0.00960163; loss: 2.78; accuracy: 82.08\n",
            "epoch: 4;batch: 26; sigma: 0.00959999; loss: 3.33; accuracy: 78.53\n",
            "epoch: 4;batch: 27; sigma: 0.00959835; loss: 2.89; accuracy: 81.37\n",
            "epoch: 4;batch: 28; sigma: 0.00959671; loss: 3.36; accuracy: 78.26\n",
            "epoch: 4;batch: 29; sigma: 0.00959508; loss: 3.33; accuracy: 78.04\n",
            "epoch: 4;batch: 30; sigma: 0.00959345; loss: 3.32; accuracy: 78.16\n",
            "epoch: 4;batch: 31; sigma: 0.00959181; loss: 3.77; accuracy: 75.29\n",
            "epoch: 4;batch: 32; sigma: 0.00959018; loss: 3.07; accuracy: 79.80\n",
            "epoch: 4;batch: 33; sigma: 0.00958855; loss: 2.87; accuracy: 81.49\n",
            "epoch: 4;batch: 34; sigma: 0.00958692; loss: 3.26; accuracy: 79.02\n",
            "epoch: 4;batch: 35; sigma: 0.0095853; loss: 2.74; accuracy: 81.86\n",
            "epoch: 4;batch: 36; sigma: 0.00958367; loss: 3.58; accuracy: 76.89\n",
            "epoch: 4;batch: 37; sigma: 0.00958205; loss: 3.05; accuracy: 80.20\n",
            "epoch: 4;batch: 38; sigma: 0.00958042; loss: 3.42; accuracy: 78.26\n",
            "epoch: 4;batch: 39; sigma: 0.0095788; loss: 3.29; accuracy: 78.92\n",
            "epoch: 4;batch: 40; sigma: 0.00957718; loss: 3.39; accuracy: 78.04\n",
            "epoch: 4;batch: 41; sigma: 0.00957556; loss: 3.57; accuracy: 76.89\n",
            "epoch: 4;batch: 42; sigma: 0.00957394; loss: 3.19; accuracy: 79.41\n",
            "epoch: 4;batch: 43; sigma: 0.00957232; loss: 3.21; accuracy: 78.94\n",
            "epoch: 4;batch: 44; sigma: 0.00957071; loss: 3.41; accuracy: 77.75\n",
            "epoch: 4;batch: 45; sigma: 0.00956909; loss: 3.47; accuracy: 77.65\n",
            "epoch: 4;batch: 46; sigma: 0.00956748; loss: 3.35; accuracy: 78.06\n",
            "epoch: 4;batch: 47; sigma: 0.00956587; loss: 3.18; accuracy: 79.41\n",
            "epoch: 4;batch: 48; sigma: 0.00956425; loss: 3.51; accuracy: 77.47\n",
            "10000/10000 [==============================] - 1s 99us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ulGVd2z0xpz",
        "colab_type": "code",
        "outputId": "ec2c74e4-e14d-480b-c36e-2f880851394a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model=vec_to_nw(y_current_best,ind,model)\n",
        "test_result=model.evaluate(x=X_test,y=y_test)\n",
        "print(test_result[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 104us/step\n",
            "0.7983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuTh7zgl2mc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "execution_time=1722.666\n",
        "file_name='nonevo8.npz'\n",
        "np.savez(file_name,run_settings=run_settings,log_train=log_train,log_validation=log_validation,test_result=test_result,evo_paths=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQofZW6s3sKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyxTznrw_qHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "count_less=0\n",
        "count_more=0\n",
        "for i in range(int(len(log_diff)/3)):\n",
        "  if i!=0:\n",
        "    if log_diff[3*i+2]<90:\n",
        "      count_less=count_less+1\n",
        "      #plt.scatter(i,-1)\n",
        "    else:\n",
        "      count_more=count_more+1\n",
        "      #plt.scatter(i,1)\n",
        "    plt.scatter(i,count_more/count_less)\n",
        "plt.show()\n",
        "print(count_less,count_more)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvLVoM7oxKhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "count_less=0\n",
        "count_more=0\n",
        "f1=plt.figure(1)\n",
        "f2=plt.figure(2)\n",
        "for i in range(int(len(log_diff)/3)):\n",
        "  if i!=0:\n",
        "    if log_diff[3*i+2]<90:\n",
        "      count_less=count_less+1\n",
        "      #plt.scatter(i,-1)\n",
        "    else:\n",
        "      count_more=count_more+1\n",
        "      #plt.scatter(i,1)\n",
        "    f1\n",
        "    plt.scatter(i,count_more,)\n",
        "    f2\n",
        "    plt.scatter(i,count_less)\n",
        "f1\n",
        "plt.show()\n",
        "f2\n",
        "plt.show()\n",
        "print(count_less,count_more)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzttRVvTTN28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(int(len(log_train)/5)):\n",
        "  plt.scatter(i*10,1000*log_train[i*5+2])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0bxUGo1U79Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(int(len(log_train)/5)):\n",
        "  plt.scatter(i,log_train[i*5+4])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JON5r3U3Cpf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(int(len(log_validation)/3)):\n",
        "  plt.scatter(i,log_validation[3*i+2])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9FJx7KIfh3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_diff2=np.empty((0,0))\n",
        "for i in range(int(len(log_diff)/3)):\n",
        "  if i!=0:\n",
        "    log_diff2=np.append(log_diff2,log_diff[i*3+2])\n",
        "print(np.amax(log_diff2))\n",
        "print(np.amin(log_diff2))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}