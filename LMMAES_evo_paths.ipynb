{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LMMAES_evo_paths.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alsr96/LMMAES/blob/master/LMMAES_evo_paths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzZwhinw1APU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import backend as K\n",
        "\n",
        "class structtype():\n",
        "    pass\n",
        "\n",
        "def nw_to_vec(model,layer_idx=None):\n",
        "    n_layers = len(model.layers)\n",
        "    vector=np.empty((0,))\n",
        "    ind=np.zeros((1,))\n",
        "    sum_i=0\n",
        "    if layer_idx==None:\n",
        "        idx=range(n_layers)\n",
        "    else:\n",
        "        idx=layer_idx\n",
        "    for i in idx:\n",
        "        if len(model.layers[i].get_weights())==2:\n",
        "            weights, biases = model.layers[i].get_weights()\n",
        "            s_w=np.size(weights)\n",
        "            sum_i=sum_i+s_w\n",
        "            ind=np.append(ind,sum_i)\n",
        "            w_v=np.reshape(weights,(s_w,))\n",
        "            s_b=np.size(biases)\n",
        "            sum_i=sum_i+s_b\n",
        "            ind=np.append(ind,sum_i)\n",
        "            b_v=np.reshape(biases,(s_b,))\n",
        "            wb=np.append(w_v,b_v)\n",
        "            vector=np.append(vector,wb)\n",
        "    return vector, ind\n",
        "\n",
        "def vec_to_nw(vector,ind,model,layer_idx=None):\n",
        "    n_layers = len(model.layers)\n",
        "    if layer_idx==None:\n",
        "        idx=range(n_layers)\n",
        "    else:\n",
        "        idx=layer_idx\n",
        "    k=0\n",
        "    for i in idx:\n",
        "        if len(model.layers[i].get_weights())==2:\n",
        "            weights,biases=model.layers[i].get_weights()\n",
        "            j1=k\n",
        "            j2=k+1\n",
        "            j3=k+2\n",
        "            weights=np.reshape(vector[int(ind[j1]):int(ind[j2])],np.shape(weights))\n",
        "            biases=np.reshape(vector[int(ind[j2]):int(ind[j3])],np.shape(biases))\n",
        "            model.layers[i].set_weights((weights,biases))\n",
        "            k=k+2\n",
        "    return model\n",
        "\n",
        "#split=[15,15]\n",
        "def split_data(input_data,target2,split):\n",
        "    n_samples=len(target2)\n",
        "    s1_input=input_data[0:int(split[0]*n_samples/(split[0]+split[1]))]\n",
        "    s1_target=target2[0:int(split[0]*n_samples/(split[0]+split[1]))]\n",
        "    s2_input=input_data[int(split[0]*n_samples/(split[0]+split[1])):n_samples]\n",
        "    s2_target=target2[int(split[0]*n_samples/(split[0]+split[1])):n_samples]\n",
        "    return s1_input, s1_target, s2_input, s2_target\n",
        "\n",
        "# obtain gradients\n",
        "def get_gradients(model,inputv,output):\n",
        "  grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
        "  symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
        "  f = K.function(symb_inputs, grads)\n",
        "  x, y, sample_weight = model._standardize_user_data(inputv, output)\n",
        "  output_grad = f(x + y + sample_weight)\n",
        "  return output_grad\n",
        "\n",
        "class lmmaes(object):\n",
        "    def __init__(self,model,inputv,output,n_candidates = None,sigma=1/10,function_budget=10000,function_target=None,layer_idx=None):\n",
        "\n",
        "      self.sigma=sigma\n",
        "      # converting the weights and biases to a row vector\n",
        "      self.layer_idx=layer_idx\n",
        "      self.y, self.ind = nw_to_vec(model,layer_idx=self.layer_idx)\n",
        "      \n",
        "      # number of layers to optimise\n",
        "      if self.layer_idx==None:\n",
        "          self.n_layers = len(model.layers)\n",
        "          itr = range(self.n_layers)\n",
        "      else:\n",
        "          self.n_layers = len(self.layer_idx)\n",
        "          itr = self.layer_idx\n",
        "\n",
        "      # calculating number of dimensions\n",
        "      self.n_dimensions=0\n",
        "      for i in itr:\n",
        "          if len(model.layers[i].get_weights())==2:\n",
        "              weights, biases = model.layers[i].get_weights()\n",
        "              self.n_dimensions=np.size(weights)+np.size(biases)+self.n_dimensions\n",
        "\n",
        "      # number of candidate solutions generated\n",
        "      self.n_candidates=n_candidates\n",
        "      if self.n_candidates==None:\n",
        "          self.n_candidates = 4 + np.floor(3*np.log(self.n_dimensions))\n",
        "\n",
        "      # number of best solutions selected\n",
        "      self.mu = np.floor(self.n_candidates/2)\n",
        "\n",
        "      # weights for selected solutions\n",
        "      self.w = np.empty([0,0])\n",
        "      for i in range(int(self.mu)):\n",
        "          self.w = np.append(self.w,np.log(self.mu+0.5)-np.log(i+1))\n",
        "      sum_w = np.sum(self.w)\n",
        "      self.w = self.w/sum_w\n",
        "\n",
        "\n",
        "      self.mu_w = 1/(np.sum(np.square(self.w)))\n",
        "\n",
        "      # number of evolution paths\n",
        "      self.m = 4 + np.floor(3*np.log(self.n_dimensions))\n",
        "\n",
        "\n",
        "      self.c_sigma = 2*self.n_candidates/self.n_dimensions\n",
        "      self.const1=np.sqrt(self.mu_w*self.c_sigma*(2-self.c_sigma))\n",
        "\n",
        "      # learning rates\n",
        "      self.c_d = np.empty([0,0])\n",
        "      self.c_c = np.empty([0,0])\n",
        "      self.const2=np.empty([0,0])\n",
        "      for i in range(int(self.m)):\n",
        "          self.c_d = np.append(self.c_d,1/(self.n_dimensions*(1.5**i)))\n",
        "          self.c_c = np.append(self.c_c,self.n_candidates/(self.n_dimensions*(4**i)))\n",
        "          self.const2=np.append(self.const2,np.sqrt(self.mu_w*self.c_c[i]*(2-self.c_c[i])))\n",
        "\n",
        "      self.t=0\n",
        "\n",
        "      # length of evolution paths (exponentially fading record of recent most successful steps)\n",
        "      #self.p_sigma=np.empty([0,0])\n",
        "      #output_grad=get_gradients(model,inputv,output)\n",
        "      #for i in range(len(output_grad)):\n",
        "      #  self.p_sigma=np.append(self.p_sigma,output_grad[i].reshape(np.size(output_grad[i]),))\n",
        "      self.p_sigma = np.zeros((self.n_dimensions,))\n",
        "\n",
        "      # vectors modelling deviation of transformation matrix from identity matrix\n",
        "      self.m_i = np.zeros((int(self.m), self.n_dimensions))\n",
        "    \n",
        "    def train_on_batch(self,model,train_data,divide_data=False):\n",
        "      self.func_calls=0\n",
        "      #t, y, p_sigma, sigma = lm_func.step_lmmaes(t,n_dimensions,m,n_candidates,c_c,c_d,loss_func,y,sigma,mu,w,c_sigma,const1,const2,data)\n",
        "      z=np.random.randn(int(self.n_candidates),self.n_dimensions)\n",
        "      d = np.copy(z)\n",
        "      f_list=np.empty((int(self.m),1))\n",
        "      for i in range(int(self.m)):\n",
        "          for j in range(np.minimum(self.t,int(self.m))):\n",
        "              d[i]=(1-self.c_d[j])*d[i] + self.c_d[j]*np.sum(self.m_i[j]**2)*d[i]\n",
        "          \n",
        "          model=vec_to_nw(self.y+self.sigma*d[i],self.ind,model,layer_idx=self.layer_idx)\n",
        "\n",
        "          train_input=train_data[0]\n",
        "          train_target=train_data[1]\n",
        "          num_samples=len(train_target)-1\n",
        "          if divide_data==False:\n",
        "            res=model.evaluate(x=train_input,y=train_target,verbose=0)\n",
        "          else:\n",
        "            res=model.evaluate(x=train_input[int(i*num_samples/self.m):int((i+1)*num_samples/self.m)],y=train_target[int(i*num_samples/self.m):int((i+1)*num_samples/self.m)],verbose=0)\n",
        "          self.func_calls=self.func_calls+1\n",
        "          f_list[i][0] = res[0]\n",
        "\n",
        "      sortidx_f = f_list.argsort(axis=0)\n",
        "\n",
        "      sortidx_f = sortidx_f[0:int(self.mu)]\n",
        "      best_list = np.empty([int(self.mu),self.n_dimensions])\n",
        "      j = 0\n",
        "      for i in sortidx_f:\n",
        "          best_list[j] = self.w[j]*d[i]\n",
        "          j = j+1\n",
        "      y_next = self.y + self.sigma*np.sum(best_list,0)\n",
        "\n",
        "      best_list2 = np.empty([int(self.mu),self.n_dimensions])\n",
        "      j = 0\n",
        "      for i in sortidx_f:\n",
        "          best_list2[j] = self.w[j]*z[i]\n",
        "          j = j+1\n",
        "      \n",
        "      p_sigma_add=np.empty([0,0])\n",
        "      output_grad=get_gradients(model,train_input,train_target)\n",
        "      for i in range(len(output_grad)):\n",
        "        p_sigma_add=np.append(p_sigma_add,output_grad[i].reshape(np.size(output_grad[i]),))\n",
        "      \n",
        "      p_sigma_next = (1-self.c_sigma)*self.p_sigma + p_sigma_add\n",
        "      mag_p_sigma_next = np.linalg.norm(p_sigma_next)\n",
        "      for i in range(int(self.m)):\n",
        "          self.m_i[i] = (1-self.c_c[i])*self.m_i[i] + self.const2[i]*np.sum(best_list2,0)\n",
        "      sigma_next = self.sigma*np.exp(self.c_sigma*(((mag_p_sigma_next**2)/self.n_dimensions)-1)/2)\n",
        "      self.t=self.t+1\n",
        "      self.sigma = sigma_next\n",
        "      self.p_sigma = p_sigma_next\n",
        "      self.y = y_next\n",
        "      model=vec_to_nw(self.y,self.ind,model,layer_idx=self.layer_idx)\n",
        "      jhlk=sortidx_f[0][0]\n",
        "      print(f_list[int(jhlk)],self.t,self.sigma)\n",
        "      return model, self.func_calls, self.sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbW11aEj2ORF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "from keras.datasets import mnist\n",
        "#download mnist data and split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train=X_train.reshape(60000,28,28,1)\n",
        "X_test=X_test.reshape(10000,28,28,1)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "638GI_Lr2Hjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "#compile model using accuracy to measure model performance\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "xtrain,ytrain,xval,yval=split_data(X_train,y_train,[5,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97lkRphBD5-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "0b3eb06b-dbbb-41d9-f217-7a5c968372b6"
      },
      "source": [
        "import numpy as np\n",
        "lm=lmmaes(model,xtrain,ytrain,n_candidates = None,sigma=1/100,layer_idx=None)\n",
        "print(lm.n_candidates)\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "res_current_best=model.evaluate(x=xval,y=yval)\n",
        "y_current_best=nw_to_vec(model,layer_idx=None)\n",
        "validation_fail=0\n",
        "\n",
        "for i in range(1):\n",
        "  # current best solution\n",
        "  \n",
        "  #xtrain,ytrain=unison_shuffled_copies(xtrain,ytrain)\n",
        "  batch=int(1000)\n",
        "  print(batch/lm.n_candidates)\n",
        "  no_samples=len(ytrain)-1\n",
        "  n_groups=int(no_samples/batch)\n",
        "  \n",
        "  for j in range(n_groups):\n",
        "    # 1 step of optimisation\n",
        "    model, func_calls, sigma = lm.train_on_batch(model,train_data=(xtrain[int(j*no_samples/n_groups):int((j+1)*no_samples/n_groups)],ytrain[int(j*no_samples/n_groups):int((j+1)*no_samples/n_groups)]),divide_data=False)\n",
        "  \n",
        "  # new solution\n",
        "  y_new,ind=nw_to_vec(model,layer_idx=None)\n",
        "  res_new=model.evaluate(x=xval,y=yval)\n",
        "  print(res_new[1],res_current_best[1])\n",
        "  \n",
        "  # validation\n",
        "  if res_new[1]>res_current_best[1]:\n",
        "    res_current_best=res_new\n",
        "    y_current_best=y_new\n",
        "    validation_fail=0\n",
        "  else:\n",
        "    model=vec_to_nw(y_current_best,ind,model,layer_idx=None)\n",
        "    validation_fail=validation_fail+1\n",
        "  \n",
        "  if validation_fail>=6:\n",
        "    break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40.0\n",
            "10000/10000 [==============================] - 6s 584us/step\n",
            "25.0\n",
            "[12.72743938] 1 0.009998099598372919\n",
            "[12.50035376] 2 0.009996236355074671\n",
            "[12.25078573] 3 0.009994505718886255\n",
            "[11.61682838] 4 0.009992971577433273\n",
            "[11.09501905] 5 0.009991566839590986\n",
            "[11.12395012] 6 0.00999051448410141\n",
            "[10.19801176] 7 0.009989514364503628\n",
            "[10.30637033] 8 0.00998867369346971\n",
            "[10.23756462] 9 0.009987960203197898\n",
            "[9.67799238] 10 0.009987603274644213\n",
            "[9.34611914] 11 0.009987715524446513\n",
            "[9.5652868] 12 0.009988106759716302\n",
            "[9.6931947] 13 0.009988717317471068\n",
            "[9.49166686] 14 0.009989632305654621\n",
            "[8.48720883] 15 0.009990648672872122\n",
            "[7.96050838] 16 0.009991730357617032\n",
            "[8.30940911] 17 0.009992948434391693\n",
            "[8.06754724] 18 0.00999450790884119\n",
            "[7.50375308] 19 0.009996426911244648\n",
            "[7.69662439] 20 0.009998687266917616\n",
            "[7.33677328] 21 0.010001299288173037\n",
            "[6.32226282] 22 0.010004320904383097\n",
            "[7.21495565] 23 0.01000772425631196\n",
            "[6.90141398] 24 0.010011713012728138\n",
            "[6.61898367] 25 0.01001621728519207\n",
            "[6.4724273] 26 0.010021238225234826\n",
            "[6.67009572] 27 0.01002657615453147\n",
            "[6.1506421] 28 0.010032231576618934\n",
            "[6.17822131] 29 0.010038433089483968\n",
            "[6.29057796] 30 0.010045035655058258\n",
            "[6.54504956] 31 0.010051758713325627\n",
            "[6.77681239] 32 0.010058710060434371\n",
            "[6.04706891] 33 0.010065822934272845\n",
            "[5.8992972] 34 0.010073270920692944\n",
            "[6.58333004] 35 0.01008107796441647\n",
            "[6.03131674] 36 0.010089538054764428\n",
            "[5.81903238] 37 0.010098273463976718\n",
            "[5.81494473] 38 0.010107437862226952\n",
            "[5.29319396] 39 0.01011700139819903\n",
            "[5.74763203] 40 0.010126892852982404\n",
            "[5.82718116] 41 0.010137227231614011\n",
            "[5.81454599] 42 0.010148068806803632\n",
            "[5.42287104] 43 0.010159117928307742\n",
            "[5.06956016] 44 0.010170671042997343\n",
            "[5.45325887] 45 0.01018243295350385\n",
            "[5.16637656] 46 0.010194581104781258\n",
            "[5.13182057] 47 0.01020713214760304\n",
            "[4.90975258] 48 0.010220038741591934\n",
            "[5.63722931] 49 0.010233238077873583\n",
            "10000/10000 [==============================] - 6s 580us/step\n",
            "0.6832 0.0825\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}